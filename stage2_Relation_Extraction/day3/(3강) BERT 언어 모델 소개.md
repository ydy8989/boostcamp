# (3ê°•) BERT ì–¸ì–´ ëª¨ë¸ ì†Œê°œ

**ê°•ì˜ ì†Œê°œ**

BERTëŠ” Bidirectional Encoder Representations from Transformersì˜ ì•½ìë¡œ êµ¬ê¸€ì´ ê³µê°œí•œ ì¸ê³µì§€ëŠ¥ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.ğŸ¤©

BERTëŠ” ì£¼ì–´ì§„ Maskì— ëŒ€í•˜ì—¬ ì–‘ë°©í–¥ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‚¬ì „ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.

ì´ë²ˆ ê°•ì˜ì—ëŠ” BERTì˜ ë‚´ë¶€ êµ¬ì¡°ì— ëŒ€í•´ ê°„ëµí•˜ê²Œ ì•Œì•„ë³´ê³ , BERTë¥¼ í™œìš©í•˜ì—¬ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ Taskì— ëŒ€í•˜ì—¬ ì•Œì•„ë´…ë‹ˆë‹¤.ğŸ¤©

 

**ì‹¤ìŠµ ì½”ë“œ ë§í¬**

- [0_Huggingface](https://drive.google.com/file/d/1YjfwILfFFQXZM-jpakRFUicMsSPOAhIw/view?usp=sharing)
- [1_BERT_ì±—ë´‡](https://drive.google.com/file/d/1WzBpwZLzHltkiwUVjpUjK7la2c60S4lI/view?usp=sharing)





## Bert ëª¨ë¸

### BERT ëª¨ë¸ ì†Œê°œ 

![image](https://user-images.githubusercontent.com/38639633/114668273-81213300-9d3b-11eb-8474-9e1b98dbc665.png)

- ìœ„ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ì€ ë°œì „ë˜ì–´ ì™”ë‹¤. 
- ê·¸ë ‡ë‹¤ë©´ bertë¥¼ ì•Œì•„ë³´ê¸° ì „ì— autoencoderë¥¼ ì‚´í´ë³´ì



#### bert

![image](https://user-images.githubusercontent.com/38639633/114979717-544d5700-9ec6-11eb-80aa-65ffd0d6e925.png)

- ì¸ì½”ë”ì™€ ë””ì½”ë”ë¡œ êµ¬ì„±ëœ ì˜¤í† ì¸ì½”ë”ëŠ” ì…ë ¥ëœ ì´ë¯¸ì§€ë¥¼ ì••ì¶•ëœ í˜•íƒœë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤. 
- ì¼ì¢…ì˜ context vector
- ì—¬ê¸°ì„œ decoderì˜ ëª©ì ì€ ë‹¤ì‹œ ì›ë³¸ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤. 
- inputì„ compressed dataë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤. 
- ê·¸ë ‡ë‹¤ë©´ **bert**ëŠ” ì–´ë–¨ê¹Œ?
- ì˜¤í† ì¸ì½”ë”ì™€ input, outputì˜ ê´€ì ì—ì„œ ë´¤ì„ ë•Œ ì°¨ì´ì ì€ **masked**ì˜ ì‚¬ìš© ìœ ë¬´ì´ë‹¤. 
- ìì—°ì–´ ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ, ì›ë³¸ì´ ì•„ë‹ˆë¼ ì¤‘ê°„ ì¤‘ê°„ì— ë§ˆìŠ¤í‚¹ì´ ë˜ì–´ìˆëŠ” ë°ì´í„°ë¥¼ ë„£ê³  ë³µì›í•˜ëŠ” ì‘ì—…ì„ í†µí•´ í•™ìŠµì´ ë” ì–´ë ¤ì›Œì§€ê²Œ ë§Œë“ ë‹¤.



#### GPTì™€ BERT

![image](https://user-images.githubusercontent.com/38639633/114981826-8ca26480-9ec9-11eb-9b23-6934395a8c28.png)

- ìœ„ì™€ ê°™ì€ ì°¨ì´ë¥¼ ê°€ì§€ê³  ìˆë‹¤. 
- GPT2ëŠ” ì›ë³¸ ì´ë¯¸ì§€ë¥¼ íŠ¹ì •í•œ ì‹œí€€ìŠ¤ë¡œ ìë¥¸ë‹¤. ì´í›„ ëª¨ë¸ì„ í†µí•´ ì˜ë ¤ì§„ ë°ì´í„°ì˜ Nextë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.



#### BERT ëª¨ë¸ì˜ êµ¬ì¡°

![image](https://user-images.githubusercontent.com/38639633/114982192-19e5b900-9eca-11eb-91bb-149f8e85c49b.png)

- ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” ìœ„ì™€ ê°™ë‹¤. 
- sentence1ê³¼ sentence2ë¥¼ `sep`í† í°ìœ¼ë¡œ êµ¬ë¶„ëœë‹¤. 
- bert ë‚´ë¶€ì˜ transformerê°€ all-to-all networkë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤.
- ê·¸ë ‡ê¸°ì— `CLS` í† í°ì˜ ì¶œë ¥ ë²¡í„°ê°€ **sen1**ê³¼ **sen2**ë¥¼ í¬ê´„í•˜ê³  ìˆëŠ” ì–´ë– í•œ ë²¡í„°ë¡œ ë…¹ì•„ë“ ë‹¤ê³  ê°€ì •í•˜ê³  ìˆë‹¤. 
- ì‹¤ì œë¡œ `CLS`í† í°ì´ **sen1**ê³¼ **sen2**ë¥¼ ì˜ í‘œí˜„í•˜ê¸° ìœ„í•´ ë§ˆì§€ë§‰ì— classification layerë¥¼ ë‘ì–´ pretrainingì„ ì§„í–‰í•˜ê²Œ ëœë‹¤. 



#### í•™ìŠµ ì½”í¼ìŠ¤

ì•„ë˜ì™€ ê°™ì´ ë§ì€ ì–‘ì˜ ì½”í¼ìŠ¤ë¥¼ í•™ìŠµì— ì‚¬ìš©í•˜ì˜€ë‹¤.

- BooksCorpus (800M-words)
- English-Wikipedia-(2,500M-words-without-lists,-tables-and-headers)
- 30,000-token-vocabulary



#### ë°ì´í„°ì˜ tokenizing

- **WordPiece** tokenizing
	- í•˜ë‚˜í•˜ë‚˜ ë‹¤ ìë¥´ê³ , ë¹ˆë„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ í•©ì¹˜ë©´ì„œ í† í¬ë‚˜ì´ì§•ì„ ì‹¤í–‰í•˜ëŠ” ë°©ë²•
- He-likes-playing $\rightarrow$ He-likes-play-##ing
- ì…ë ¥ ë¬¸ì¥ì„ tokenizingí•˜ê³ ,-ê·¸ tokenë“¤ë¡œ â€˜token-sequenceâ€™ë¥¼ ë§Œë“¤ì–´ í•™ìŠµì— ì‚¬ìš©
- 2ê°œì˜ token-sequenceê°€ í•™ìŠµì— ì‚¬ìš©



#### Masked language model

![image](https://user-images.githubusercontent.com/38639633/114982982-0ab33b00-9ecb-11eb-91bb-61b4015c4970.png)

- ìœ„ì™€ ê°™ì´ ì˜¤ë¦¬ì§€ë„ sentenceê°€ ì£¼ì–´ì¡Œì„ ë•Œ, CLSì™€ SEP í† í°ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ í† í° ì¤‘ 15%ë¡œ ëœë¤í•˜ê²Œ ì„ íƒí•œë‹¤. 

![image](https://user-images.githubusercontent.com/38639633/114983084-274f7300-9ecb-11eb-8685-effbe43f0b2e.png)

- ì´ë ‡ê²Œ ì„ íƒ ëœ í† í° ì¤‘ 80%ëŠ” Maskingì— ì‚¬ìš©í•˜ê³ 
- 10%ëŠ” vocab ë‚´ì˜ ì•„ë¬´ í† í°ìœ¼ë¡œ replace í•œë‹¤. 
- ë‚˜ë¨¸ì§€ 10%ëŠ” ë°”ê¾¸ì§€ ì•ŠëŠ”ë‹¤

![image](https://user-images.githubusercontent.com/38639633/114983397-89a87380-9ecb-11eb-8889-18844bfeea04.png)

- ìµœì¢…ì ìœ¼ë¡œëŠ” ìœ„ì™€ ê°™ì€ input ë°ì´í„°ê°€ bertì— ë“¤ì–´ê°€ê²Œ ëœë‹¤. 



#### ë‹¤ì–‘í•œ NLP ì‹¤í—˜

- **GLUE datasets**
	- â€’MNLI: Multi-Genre Natural Language Inference
		- â€’ë‘ë¬¸ì¥ì˜ê´€ê³„ë¶„ë¥˜ë¥¼ìœ„í•œë°ì´í„°ì…‹
	- â€’QQP: Quora Question Pairs
		- â€’ë‘ì§ˆë¬¸ì´ì˜ë¯¸ìƒê°™ì€ì§€ë‹¤ë¥¸ì§€ë¶„ë¥˜ë¥¼ìœ„í•œë°ì´í„°ì…‹
	- â€’QNLI: Question Natural Language Inference
		- â€’ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹
	- â€’SST-2 : The Stanford Sentiment Treebank
		- â€’ì˜í™” ë¦¬ë·° ë¬¸ì¥ì— ê´€í•œ ê°ì„±ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ì…‹
	- â€’CoLA : The Corpus of Linguistic Acceptability
		- â€’ë¬¸ë²•ì ìœ¼ë¡œ ë§ëŠ” ë¬¸ì¥ì¸ì§€ í‹€ë¦°ë¬¸ì¥ì¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹
	- â€’STS-B : The Semantic Textual Similarity Benchmark
		- â€’ë‰´ìŠ¤ í—¤ë“œë¼ì¸ê³¼ ì‚¬ëŒì´ ë§Œë“  paraphrasing1 ë¬¸ì¥ì´ ì˜ë¯¸ìƒ ê°™ì€ ë¬¸ì¥ì¸ì§€ ë¹„êµë¥¼ ìœ„í•œ ë°ì´í„°ì…‹
	- â€’MRPC : Microsoft Research Paraphrase Corpus
		- â€’ë‰´ìŠ¤ì˜ë‚´ìš©ê³¼ì‚¬ëŒì´ë§Œë“ ë¬¸ì¥ì´ì˜ë¯¸ìƒê°™ì€ë¬¸ì¥ì¸ì§€ë¹„êµë¥¼ìœ„í•œë°ì´í„°ì…‹
	- â€’RTE : Recognizing Textual Entailment
		- â€’MNLIì™€ ìœ ì‚¬í•˜ë‚˜,  ìƒëŒ€ì ìœ¼ë¡œ í›¨ì”¬ ì ì€ í•™ìŠµ ë°ì´í„°ì…‹
	- â€’WNLI : Winograd NLI
		- â€’ë¬¸ì¥ ë¶„ë¥˜ ë°ì´í„°ì…‹
- **SQuAD v1.1%ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹**
- **CoNLL 2003%Named-Entity-Recognition-datasets** 
	- ê°œì²´ëª… ë¶„ë¥˜ ë°ì´í„°ì…‹
- **SWAG:-Situations-With-Adversarial-Generations** 
	- â€’í˜„ì¬ ë¬¸ì¥ ë‹¤ìŒì— ì´ì–´ì§ˆ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ì„ íƒí•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹

ì´ëŸ¬í•œ ë°ì´í„°ì…‹ë“¤ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ì„±ëŠ¥ í‰ê°€ì˜ ì •í™•ì„±ê³¼ ê°ê´€ì„±ì„ ë³´ì¥ë°›ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. 



**NLP ì‹¤í—˜**

ìœ„ì˜ ë°ì´í„° ì…‹ìœ¼ë¡œ ì•„ë˜ì˜ taskë“¤ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. 

![image](https://user-images.githubusercontent.com/38639633/114984346-9a0d1e00-9ecc-11eb-806d-fd110f68519f.png)

- ë‹¨ì¼ë¬¸ì¥
	- bert ëª¨ë¸ì— í•œ ê°œì˜ ë¬¸ì¥ì´ ì…ë ¥ ë˜ì—ˆì„ ë•Œ ë¶„ë¥˜í•˜ëŠ” taskì´ë‹¤

- ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜
	- ì…ë ¥ìœ¼ë¡œ ë‘ ë¬¸ì¥ì´ ë“¤ì–´ê°€ëŠ” ê²½ìš°ì´ë‹¤. 
	- ë‹¤ìŒ ë¬¸ì¥ì„ ì˜ˆì¸¡í•˜ëŠ” ê²½ìš°, sentence1ì´ sentence2ì˜ ê°€ì„¤ì´ ëœë‹¤ê±°ë‚˜, ìœ ì‚¬ë„ë¥¼ ë¹„êµí•  ìˆ˜ ìˆë‹¤
	- í˜¹ì€ paraphraseëœ ê²ƒì„ detectioní•˜ëŠ” taskì— ì ìš©í•  ìˆ˜ë„ ìˆë‹¤.
- ë¬¸ì¥ í† í° ë¶„ë¥˜ 
	- ê°œì²´ëª… ì¸ì‹ì´ ê·¸ ì˜ˆì‹œì´ë‹¤. 
	- output token ê°ê°ì˜ ìœ„ì— token classifierë¥¼ ë¶€ì°©í•¨ìœ¼ë¡œì¨ í† í°ì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì„ ì§„í–‰
- ê¸°ê³„ ë…í•´ ì •ë‹µ ë¶„ë¥˜
	- ë‘ ê°€ì§€ ì •ë³´ê°€ ì£¼ì–´ì§„ë‹¤.(sentence1:ì§ˆë¬¸, sentence2 : í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µì´ í¬í•¨ëœ ë¬¸ì¥)
	- sentence2ì˜ ìˆ˜ë§ì€ í† í° ì¤‘ì—ì„œ start pointì™€ end pointì˜ ìœ„ì¹˜ë¥¼ ì¡ì•„ë‚´ì£¼ëŠ” taskì´ë‹¤. 

- ì´ ëª¨ë“  taskê°€ ë‹¬ë¼ ë³´ì´ì§€ë§Œ, ì‚¬ì‹¤ì€ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šë‹¤. 



### BERT ëª¨ë¸ì˜ ì‘ìš©

#### Single sentence classification

![image](https://user-images.githubusercontent.com/38639633/115987535-65961200-a5f0-11eb-9ad2-a5c8bf60b0b1.png)

ë‹¤ìŒì˜ ë‘ taskëŠ” inputì´ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ì— ì†í•˜ëŠ” taskì´ë‹¤.



**1. ê°ì„±ë¶„ì„**

ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ì½”í¼ìŠ¤([https://github.com/e9t/nsmc](https://github.com/e9t/nsmc))ë¡œ ê°ì„±ë¶„ì„

í•™ìŠµ : 150000ë¬¸ì¥ / í‰ê°€ : 50000ë¬¸ì¥ (ê¸ì •1, ë¶€ì • 0)

![image](https://user-images.githubusercontent.com/38639633/114985521-de4cee00-9ecd-11eb-9136-647327c2f865.png)

- í•˜ë‚˜ì˜ single sentenceë¥¼ ì…ë ¥ë°›ì•„ ì£¼ì–´ì§„ ë¬¸ì¥ì´ ê¸ì •ì¸ì§€ ë¶€ì •ì¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” taskì´ë‹¤. 
- input : í•˜ë‚˜ì˜ sentence + ê°ê°ì˜ label

	

**2. ê´€ê³„ ì¶”ì¶œ**

![image](https://user-images.githubusercontent.com/38639633/115987353-a9d4e280-a5ef-11eb-964b-bf149a7270e5.png)

- ë¬¸ì¥ ë‚´ entityì˜ ê´€ê³„ë¥¼ ë¶„ë¥˜í•˜ëŠ” task
- single sentence classificationì¸ ì´ìœ ëŠ” ë‘ ê°œì˜ entityì™€ sentence í•˜ë‚˜ê°€ í•©ì³ì„œ 1ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ inputë˜ê¸° ë•Œë¬¸ì´ë‹¤. 



#### Sentence pair classification

![image](https://user-images.githubusercontent.com/38639633/115987614-c4f42200-a5f0-11eb-9988-e58c22d28049.png)

**ì˜ë¯¸ ë¹„êµ**

ë””ì§€í„¸ ë™ë°˜ì íŒ¨ëŸ¬í”„ë ˆì´ì§• ì§ˆì˜ ë¬¸ì¥ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ì§ˆë¬¸-ì§ˆë¬¸ ë°ì´í„° ìƒì„± ë° í•™ìŠµ
í•™ìŠµ:\3,401\ë¬¸ì¥ ìŒ (ìœ ì‚¬ X:\1,700ê°œ,\ìœ ì‚¬ O:\1,701ê°œ)
í‰ê°€:\1,001\ë¬¸ì¥ ìŒ (ìœ ì‚¬ X:\500ê°œ,\ìœ ì‚¬ O:\501ê°œ)

![image](https://user-images.githubusercontent.com/38639633/115987433-16e87800-a5f0-11eb-946b-d5372ad91968.png)

- ë‘ ë¬¸ì¥ì´ ì˜ë¯¸ì ìœ¼ë¡œ ê°™ëƒ vs ë‹¤ë¥´ëƒë¥¼ ë¶„ë¥˜í•˜ëŠ” taskì´ë‹¤. 
- sentence1ê³¼ sentence2 ë‘ ê°œì˜ ë¬¸ì¥ì´ inputìœ¼ë¡œ ë“¤ì–´ê°€ë©°, ë‘ ë¬¸ì¥ì˜ ì˜ë¯¸ê°€ ê°™ì€ì§€ ì—¬ë¶€ë¥¼ binary classificationí•˜ëŠ” ë¬¸ì œì´ë‹¤. 
- ì—¬ê¸°ì„œ ê³ ë ¤í•´ì•¼í•˜ëŠ” ë¬¸ì œëŠ” ë°ì´í„° ì „ì²˜ë¦¬ì˜ ë¬¸ì œì´ë‹¤. 
	- `label 0`ìœ¼ë¡œ ë˜ì–´ìˆëŠ” ê´€ê³„ì—†ëŠ” ë¬¸ì¥ì˜ ê²½ìš°, ë„ˆë¬´ ìƒê´€ì—†ëŠ”  ë‘ ë¬¸ì¥ìœ¼ë¡œ ì´ë£¨ì–´ì¡Œì„ ë•Œ ë‹¤ë¥¸ ê³³ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ëª¨ë¸ì´ ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. 
	- ì‹¤ì œ ë¶„ë¥˜í•´ì•¼í•˜ëŠ” ë¬¸ì¥ì€ êµ‰ì¥íˆ ì˜ë¯¸ê°€ ìœ ì‚¬í•˜ê¸° ë•Œë¬¸ì—, ì´ë ‡ê²Œ ë„ˆë¬´ë„ ë‹¤ë¥¸ ë‘ ë¬¸ì¥ì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ë³„ ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤. 



#### Sentence token classification

![image](https://user-images.githubusercontent.com/38639633/115987594-a2620900-a5f0-11eb-81d2-243f35290852.png)

ë¬¸ì¥ì˜ tokenì„ ë¶„ë¥˜í•˜ëŠ” taskì´ë‹¤. 



**ê°œì²´ëª… ë¶„ì„**

ETRI\ê°œì²´ëª… ì¸ì‹ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ë° í‰ê°€ ì§„í–‰ (ì •ë³´í†µì‹ ë‹¨ì²´í‘œì¤€ TTA.KO-10.0852)
í•™ìŠµ:\95,787\ë¬¸ì¥ /\í‰ê°€:\10,503\ë¬¸ì¥

![image](https://user-images.githubusercontent.com/38639633/115987637-dfc69680-a5f0-11eb-9e1a-7df285b84af4.png)

- bert ëª¨ë¸ì„ í†µê³¼í•˜ë©´ì„œ ê° í† í°ë³„ë¡œ ë‚˜ì˜¤ëŠ” outputê³¼ labelì„ ë§¤ì¹­ì‹œì¼œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ë‹¤. 
- ìœ„ì˜ ë‘ ë°©ì‹(1sentence classificataionê³¼ 2sentence classification)ì€ ë§ˆì§€ë§‰ layerì˜ cls í† í°ì—ë§Œ classifierë¥¼ ë¶™ì—¬ ë¶„ë¥˜í•˜ëŠ” ë°©ì‹ì´ì—ˆì§€ë§Œ, ê·¸ë¦¼ê³¼ ê°™ì´ ëª¨ë“  í† í°ì—ì„œì˜ label(ê°œì²´ëª…)ì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì´ ì´ë£¨ì–´ì§€ëŠ” taskì´ë‹¤. 



#### Machine Reading Comprehension(ê¸°ê³„ë…í•´)

![image](https://user-images.githubusercontent.com/38639633/115987811-9e82b680-a5f1-11eb-9f12-321bab968ef9.png)

- questionê³¼ paragraphë¥¼ inputìœ¼ë¡œ ê°–ê³  ìˆë‹¤. 

- í† í¬ë‚˜ì´ì €ì— ë”°ë¼ ì„±ëŠ¥í‰ê°€ì— ë§ì€ ì˜í–¥ì´ ìˆë‹¤. 

	![image](https://user-images.githubusercontent.com/38639633/115992218-79993e00-a607-11eb-85a8-8582d6c0e59d.png)



### í•œêµ­ì–´ BERT ëª¨ë¸

#### ETRI KoBertì˜ tokenizing

- í•œêµ­ì–´ë¡œ í•™ìŠµëœ í•œêµ­ì–´ ëª¨ë¸

- ì˜¤ëœ ê¸°ê°„ë™ì•ˆ KorQuADì—ì„œ 1ë“±ì„ ìœ ì§€í•˜ê³  ìˆë˜ ëª¨ë¸ì´ë‹¤.

- wordpieceë¥¼ ì‚¬ìš©í•˜ì˜€ì§€ë§Œ, ìŒì ˆì´ ì•„ë‹Œ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ë¥¼ í•œ ë’¤ wordpieceë¥¼ íƒœìš´ í† í¬ë‚˜ì´ì§• ë°©ì‹ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. 

	![image](https://user-images.githubusercontent.com/38639633/115992305-ef050e80-a607-11eb-8a17-a5fda4be4420.png)

- êµ¬ê¸€ì˜ base bertë³´ë‹¤ 10ì  ê°€ëŸ‰ì˜ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆë‹¤. 

- ETRI í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•´ì•¼ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 



#### í•œêµ­ì–´ í† í¬ë‚˜ì´ì§•ì— ë”°ë¥¸ ì„±ëŠ¥ë¹„êµ

> [https://arxiv.org/abs/2010.02534](https://arxiv.org/abs/2010.02534)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- í˜•íƒœì†Œ ë¶„ì„ í›„ wordpieceë¥¼ íƒœìš´ ê²ƒì´ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ë‹¤ê³  ì£¼ì¥í•˜ëŠ” ë…¼ë¬¸ì´ë‹¤. 



#### Advanced BERT model

>  KBQAì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ Entity ì •ë³´ê°€ ê¸°ì¡´ BERTì—ëŠ” í¬í•¨ë˜ì–´ìˆì§€ ì•Šë‹¤. Entity linkingì„ í†µí•œ ì£¼ìš” entity ì¶”ì¶œ ë° entity tag ë¶€ì°©ê³¼ entity embedding layer ì¶”ê°€ ê·¸ë¦¬ê³  í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•´ NNPì™€ entity ìš°ì„  chunking maskingì„ í†µí•´ KorQuADì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë°›ì„ ìˆ˜ ìˆì—ˆë‹¤. -ê¹€ì„±í˜„ ë§ˆìŠ¤í„°

![image](https://user-images.githubusercontent.com/38639633/115992491-ec56e900-a608-11eb-89d5-6cdd3eb1f66d.png)

![image](https://user-images.githubusercontent.com/38639633/115992498-fbd63200-a608-11eb-9fbf-1ae55a51a00d.png)

- ë§ˆì§€ë§‰ ì¤„ Entity Embedding layerê°€ ì¶”ê°€ë˜ì—ˆë‹¤. 
- ê·¸ ê²°ê³¼ ë‹¤ìŒì˜ ì„±ëŠ¥ì„ ì–»ì—ˆë‹¤ê³  í•œë‹¤.

![image](https://user-images.githubusercontent.com/38639633/115992559-48217200-a609-11eb-807f-15346c2662cf.png)

- 20Gì˜ í•™ìŠµë°ì´í„°ì™€ ë‹¬ë¦¬ ë” ì ì€ ëª¨ë¸ë¡œ ì ì€ í•™ìŠµì„ ì‹œì¼°ìŒì—ë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì˜€ë‹¤ê³  í•œë‹¤.
- ì–¸ì–´ ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ featureê°€ ë¬´ì—‡ì¸ì§€ë¥¼ ê³ ë¯¼í•˜ê³ , ì‚¬ëŒì´ ìì—°ì–´ ì²˜ë¦¬í•  ë•Œ ì–´ë– í•œ featureë¥¼ ì‚¬ìš©í•  ê²ƒì¸ì§€ ê·¸ë¦¬ê³  ê·¸ê²ƒì„ ëª¨ë¸ì— ë…¹ì¸ë‹¤ë©´ ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ìƒê°ëœë‹¤. 


```python
!pip install transformer

from transformers import AutoModel, AutoTokenizer, BertTokenizer
# Store the model we want to use
MODEL_NAME = "bert-base-multilingual-cased"

# We need to create the model and tokenizer
model = AutoModel.from_pretrained(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
```

- `!pip install transformers`ë¥¼ í†µí•´ ê°„ë‹¨íˆ ì„¤ì¹˜í•  ìˆ˜ ìˆë‹¤. 
- ëª¨ë¸ Nameì— ë”°ë¥¸ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°ì™€ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°ê°€ ê°€ëŠ¥í•˜ë‹¤. 
- Automodel, AutoTokenizerë¥¼ í†µí•´ ìë™ìœ¼ë¡œ ëª¨ë¸ì— ë§ëŠ” í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ì„ loadí•  ìˆ˜ ìˆë‹¤. 
	- ê¸°ì¡´ì—ëŠ” bertëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ bertmodel, bertTokenizerë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , Electra ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ElectraModel, ElcetraTokenizerë¥¼ ë¶ˆëŸ¬ì™€ì•¼ë§Œ í–ˆë‹¤ 
	- í•˜ì§€ë§Œ, ì´ë¥¼ ìë™ìœ¼ë¡œ ë§¤í•‘í•´ì£¼ëŠ” Auto - ì‹œë¦¬ì¦ˆë¥¼ í†µí•´ ì´ì œëŠ” Nameë§Œìœ¼ë¡œ ê°„ë‹¨íˆ ì§€ì •í•  ìˆ˜ ìˆë‹¤
	- (ì£¼ì˜) ê·¸ë ‡ì§€ë§Œ, í•­ìƒ ì™„ë²½í•˜ì§€ëŠ” ì•Šë‹¤. íŠ¹ì • ëª¨ë¸ì—ëŠ” ë²„ê·¸ê°€ ì¡´ì¬í•˜ì—¬ ì œëŒ€ë¡œ ë¶ˆëŸ¬ì™€ì§€ì§€ ì•ŠëŠ” í˜„ìƒì´ ê°„í˜¹ ìˆë‹¤ê³  í•œë‹¤.

```python
print(tokenizer.vocab_size)
>>> 119547
```
- 11ë§Œê°œì˜ vocabìœ¼ë¡œ ì´ë¤„ì§„ í† í¬ë‚˜ì´ì € ì‚¬ì „ì„ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 
- ìœ„ tokenizerëŠ” êµ¬ê¸€ì—ì„œ ê³µê°œí•œ ë‹¤êµ­ì–´ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ëœ 'bert-base-multilingual-cased' tokenizer ì‚¬ì „ì´ë©° ì•½ 12ë§Œê°œì˜ wordpiece í† í°ë“¤ ì¤‘ ì•½ **8ì²œê°œ** ê°€ëŸ‰ë§Œì´ í•œêµ­ì–´ì´ë‹¤. 
	- ~~ì–´ì§€ê°„í•˜ë©´ ì‚¬ìš©í•˜ì§€ë§ˆë¼...~~
- ì›Œë“œí”¼ìŠ¤ ê¸°ì¤€ vocabì„ ì •ì˜í•˜ë ¤ë©´ ì•½ 3ë§Œê°œì •ë„ë¡œ ì‚¬ì´ì¦ˆë¥¼ ì§€ì •í•˜ë©´ í•œìì–´ë„ ì¸ì‹ê°€ëŠ¥í•œ ì •ë„ì˜ ì½”í¼ìŠ¤ë¥¼ ì œì‘í•  ìˆ˜ ìˆë‹¤. 

```python
for i, key in enumerate(tokenizer.get_vocab()):
    print(key)
    if i > 20:
        break
```

```
>>>
Vol
Estadual
##Õ¥Ö€Õ¨
à¦¸à¦‚à¦¸à§à¦•à¦°à¦£
Voogd
RTL
nghá»
##à¤‚à¤—à¤¾
##ÑˆĞ¸Ğ½Ğ°
Europese
1001
##Ù‡Ø§Ø±
##ä¼˜
##à®¯à®¿à®²à¯
##lmesi
Sioux
##áƒšáƒ”
##É™tli
Â§
```



ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì§• ì˜ˆì‹œë¥¼ ì‚´í´ë³´ë©´

```python
text = "ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤."
tokenized_input_text = tokenizer(text, return_tensors="pt")
for key, value in tokenized_input_text.items():
    print("{}:\n\t{}".format(key, value))
```

```
input_ids:
	tensor([[   101,   9638, 119064,  25387,  10892,  59906,   9694,  46874,   9294,
          25387,  11925,    119,    102]])
token_type_ids:
	tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
attention_mask:
	tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
```

- tokenizerì— textë¥¼ ë„£ê³  pytorchë¡œ ë°˜í™˜í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì˜ 'pt' ì¸ìë¥¼ ë„£ì–´ì£¼ë©´ ê°ì²´ë¥¼ ìƒì„±í•œë‹¤.

```python
tokenized_text = tokenizer.tokenize(text)
print(tokenized_text)
input_ids = tokenizer.encode(text)
print(input_ids)
decoded_ids = tokenizer.decode(input_ids)
print(decoded_ids)
```

```
['ì´', '##ìˆœ', '##ì‹ ', '##ì€', 'ì¡°ì„ ', 'ì¤‘', '##ê¸°ì˜', 'ë¬´', '##ì‹ ', '##ì´ë‹¤', '.']
[101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 102]
[CLS] ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤. [SEP]
```

- `tokenizer.tokenize(text)`ë¥¼ í†µí•´ ìœ„ì™€ ê°™ì´ í† í¬ë‚˜ì´ì§•ì´ ëœ ëª¨ìŠµì„ ì§ì ‘ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 
- `.encode()`ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”© ê²°ê³¼ë¥¼ ì¶œë ¥í•´ì¤€ë‹¤. 
	- ìœ„ì—ì„œ 101ë²ˆê³¼ 102ëŠ” ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì„ ì•Œë ¤ì£¼ëŠ” í† í¬ë‚˜ì´ì €ì´ë‹¤. 
	- ì´ë¥¼ `.decode()`ë¥¼ í†µí•´ ë‹¤ì‹œ ë³€í™˜í•˜ë©´ `[cls]`ì™€ `[sep]`ë¡œ ë³€í™˜ ë˜ì–´ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 
- `tokenizer`ì˜ defaultë¡œ ì• ë’¤ì˜ ìŠ¤í˜ì…œ í† í°ì´ ë¶™ëŠ”ë‹¤. 
- ì´ê²ƒì„ ì›ì¹˜ ì•ŠëŠ”ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ `add_special_tokens=False`ì˜µì…˜ì„ í†µí•´ ë„£ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.

```python
tokenized_text = tokenizer.tokenize(text, add_special_tokens=False)
print(tokenized_text)
input_ids = tokenizer.encode(text, add_special_tokens=False)
print(input_ids)
decoded_ids = tokenizer.decode(input_ids)
print(decoded_ids)
```

```
['ì´', '##ìˆœ', '##ì‹ ', '##ì€', 'ì¡°ì„ ', 'ì¤‘', '##ê¸°ì˜', 'ë¬´', '##ì‹ ', '##ì´ë‹¤', '.']
[9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119]
ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.
```

---

íŠ¹ì • ì—­í• ì„ ìœ„í•œ special tokenë„ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤.

```python
text = "[ENTITY]ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.[/ENTITY]"
# [ENTITY]ì´ìˆœì‹ [/ENTITY]
tokenized_text = tokenizer.tokenize(text, add_special_tokens=False)
print(tokenized_text)
input_ids = tokenizer.encode(text, add_special_tokens=False)
print(input_ids)
decoded_ids = tokenizer.decode(input_ids)
print(decoded_ids)
```

```
['[', 'EN', '##TI', '##TY', ']', 'ì´', '##ìˆœ', '##ì‹ ', '##ì€', 'ì¡°ì„ ', 'ì¤‘', '##ê¸°ì˜', 'ë¬´', '##ì‹ ', '##ì´ë‹¤', '.', '[', '/', 'EN', '##TI', '##TY', ']']
[164, 38702, 59879, 11517, 166, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 164, 120, 38702, 59879, 11517, 166]
[ ENTITY ] ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤. [ / ENTITY ]
```

- ìŠ¤í˜ì…œ í† í°ì„ ì¶”ê°€í•˜ì§€ ì•Šì„ ê²½ìš°, cumstom í† í°ì€ ì˜ë¬¸ ê·¸ëŒ€ë¡œ ì¸ì‹í•˜ì—¬ ë¶„ë¦¬ëœë‹¤. 
	- `ENTITY`í† í°ì´ ê°ê° ë¶„ë¦¬ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. 
- ì•„ë˜ì™€ ê°™ì´ `add_special_toekns`ì„ í†µí•´ ìŠ¤í˜ì…œ í† í°ì„ ë“±ë¡í•  ìˆ˜ ìˆë‹¤. 

```python
text = "[SHKIM]ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.[/SHKIM]"

added_token_num += tokenizer.add_special_tokens({"additional_special_tokens":["[ENTITY]", "[/ENTITY]"]})
tokenized_text = tokenizer.tokenize(text, add_special_tokens=False)
print(tokenized_text)
input_ids = tokenizer.encode(text, add_special_tokens=False)
print(input_ids)
decoded_ids = tokenizer.decode(input_ids)
print(decoded_ids)
decoded_ids = tokenizer.decode(input_ids,skip_special_tokens=True)
print(decoded_ids)
```

```
['[ENTITY]', 'ì´', '##ìˆœ', '##ì‹ ', '##ì€', 'ì¡°ì„ ', 'ì¤‘', '##ê¸°ì˜', 'ë¬´', '##ì‹ ', '##ì´ë‹¤', '.', '[/ENTITY]']
[119552, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 119553]
[ENTITY] ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤. [/ENTITY]
ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.
```


























































