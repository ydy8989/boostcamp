# (4ê°•) í•œêµ­ì–´ BERT ì–¸ì–´ ëª¨ë¸ í•™ìŠµ

**ê°•ì˜ ì†Œê°œ**

ì´ë²ˆì—ëŠ” 3ê°•ì—ì„œ ì†Œê°œí•œ BERTë¥¼ ì§ì ‘ í•™ìŠµí•˜ëŠ” ê°•ì˜ì…ë‹ˆë‹¤.

ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ë“¤ì„ í™œìš©í•˜ê³  ê³µìœ í•  ìˆ˜ ìˆëŠ” Huggingface Hubì— ëŒ€í•´ ì†Œê°œí•˜ê³ , ì§ì ‘ ë³¸ì¸ì˜ ëª¨ë¸ì„ ê³µìœ í•˜ëŠ” ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.ğŸ¤“



## BERT í•™ìŠµí•˜ê¸°

### Bert ëª¨ë¸ í•™ìŠµ ë‹¨ê³„

1. Tokenizer ë§Œë“¤ê¸°
2. ë°ì´í„°ì…‹ í™•ë³´
3. Next sentence prediction(NSP)
4. Masking



#### ì™œ ìƒˆë¡œ í•™ìŠµí•¨? 

- ë„ë©”ì¸ íŠ¹í™” taskì˜ ê²½ìš°, í•´ë‹¹ ë„ë©”ì¸ë§Œì˜ í•™ìŠµ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤ëŠ” ì—°êµ¬ê²°ê³¼ê°€ ì´ë¯¸ ë§ì´ ë‚˜ì™€ìˆë‹¤. 

- ex) ë²•ë¥  ê´€ë ¨ ëª¨ë¸ì„ ë§Œë“¤ë•Œ, ê¸°ì¡´ì˜ bert ëª¨ë¸ì„ fine tuning í•˜ëŠ” ê²ƒë³´ë‹¤ ë²•ë¥  ê´€ë ¨ ë°ì´í„°ë§Œì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤.!!!

	![image](https://user-images.githubusercontent.com/38639633/115997004-7492b980-a61c-11eb-9687-1cab8aa05ea4.png)

- ìœ„ í‘œì—ì„œ ë§¨ ë§ˆì§€ë§‰ PubMedBERTëŠ” ìƒëª…ê³¼í•™(?), ìƒë¦¬í•™ ë¶„ì•¼ì˜ ë…¼ë¬¸ì„ ì „ì²˜ë¦¬í•œ ë°ì´í„°ë¡œ ë§Œë“  ê´€ë ¨ ëª¨ë¸ì´ë‹¤. 

	![image-20210425231910219](C:\Users\doyeon\AppData\Roaming\Typora\typora-user-images\image-20210425231910219.png)

- ìœ„ í‘œì—ì„œ ì¢Œì¸¡ì˜ ëª©ë¡ë“¤ì€ í™”í•™, ìƒëª…ê³¼í•™ ë¶„ì•¼ì˜ ìì—°ì–´ì²˜ë¦¬ taskë¥¼ ì˜ë¯¸í•œë‹¤. ëŒ€ë¶€ë¶„ì˜ ë¶„ì•¼ì—ì„œ ê°ì¢… bert pretrained modelë³´ë‹¤ PubMedBERTê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 



#### í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ë§Œë“¤ê¸°

- ì´ì²˜ëŸ¼ ì›í•˜ëŠ” ë„ë©”ì¸ì— ë§ëŠ” ë°ì´í„°ë¡œ ìƒˆë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê¸° ë•Œë¬¸ì—, í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ë¥¼ ë§Œë“¤ í•„ìš”ê°€ ìˆë‹¤. 
- bertì˜ ê¸°ë³¸ ëª¨ë¸ì˜ input í˜•ì‹ì— ë§ëŠ” `input_ids`, `token_type_ids`ë“±ì„ ë§Œë“¤ í•„ìš”ê°€ ìˆë‹¤. 
- ë˜í•œ, ì´ë ‡ê²Œ ë§Œë“  ë°ì´í„° ì…‹ì„ `masking`ì„ í†µí•´ ì–´ë– í•œ í˜•ì‹ìœ¼ë¡œ ëª¨ë¸ì— í•™ìŠµì‹œí‚¬ì§€ë¥¼ ê³ ë¯¼í•´ì•¼í•œë‹¤. 

 



**Reference**

- LM training from scratch
	- [https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=5oESe8djApQw](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=5oESe8djApQw ) 
- ë‚˜ë§Œì˜ BERT Wordpiece Vocab ë§Œë“¤ê¸°
	- [https://monologg.kr/2020/04/27/wordpiece-vocab/](https://monologg.kr/2020/04/27/wordpiece-vocab/)
	- [https://velog.io/@nawnoes/Huggingface-tokenizers%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-Wordpiece-Tokenizer-%EB%A7%8C%EB%93%A4%EA%B8%B0](https://velog.io/@nawnoes/Huggingface-tokenizersë¥¼-ì‚¬ìš©í•œ-Wordpiece-Tokenizer-ë§Œë“¤ê¸°)
- Extracting training data from large language model
	- [https://www.youtube.com/watch?v=NGoDUEz3tZg](https://www.youtube.com/watch?v=NGoDUEz3tZg)
- BERT ì¶”ê°€ ì„¤ëª…
	- [https://jiho-ml.com/weekly-nlp-28/](https://jiho-ml.com/weekly-nlp-28/)

