---
layout: post
title: Transformer I
subtitle: ê·¸ì•¼ë§ë¡œ Attention is all you need
thumbnail-img : https://user-images.githubusercontent.com/38639633/108290231-5307f280-71d3-11eb-9576-f3cf9eca37a0.png
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [BOOSTCAMP]
tags: [boostcamp,transformer, machine translation]
comments: true
---

ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” í˜„ì¬ NLP ì—°êµ¬ ë¶„ì•¼ì—ì„œ ê°€ì¥ ë§ì´ í™œìš©ë˜ê³  ìˆëŠ” Transformer(Self-Attention)ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë´…ë‹ˆë‹¤. Self-Attentionì€ RNN ê¸°ë°˜ ë²ˆì—­ ëª¨ë¸ì˜ ë‹¨ì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì²˜ìŒ ë“±ì¥í–ˆìŠµë‹ˆë‹¤. RNNê³¼ Attentionì„ í•¨ê»˜ ì‚¬ìš©í–ˆë˜ ê¸°ì¡´ê³¼ëŠ” ë‹¬ë¦¬ Attention ì—°ì‚°ë§Œì„ ì´ìš©í•´ ì…ë ¥ ë¬¸ì¥/ë‹¨ì–´ì˜ representationì„ í•™ìŠµì„ í•˜ë©° ì¢€ ë” parallelí•œ ì—°ì‚°ì´ ê°€ëŠ¥í•œ ë™ì‹œì— í•™ìŠµ ì†ë„ê°€ ë¹ ë¥´ë‹¤ëŠ” ì¥ì ì„ ë³´ì˜€ìŠµë‹ˆë‹¤

**Further Reading**

- [Attention is all you need, NeurIPS'17](https://arxiv.org/abs/1706.03762)
- [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)



# Transformer I

> ê³¼ê±° Attention is all you need ë…¼ë¬¸ì„ [í¬ìŠ¤íŒ…](https://ydy8989.github.io/2021-01-10-transformer/)í–ˆë˜ ì ì´ ìˆì§€ë§Œ, naver boostcamp ê³¼ì •ì„ ìˆ˜ê°•í•˜ë©´ì„œ ë‹¤ì‹œ í•œ ë²ˆ ë“±ì¥í•œ transformerì— ëŒ€í•´ í¬ìŠ¤íŒ… í•˜ë ¤ê³  í•œë‹¤. ì§€ë‚œë²ˆì—” ë…¼ë¬¸ì˜ íë¦„ì— ë”°ë¼ ì„¤ëª…ì„ ì§„í–‰í–ˆë‹¤ë©´ ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì¡°ê¸ˆ ë” ì‹¤ì§ˆì ì´ê³  ì‚¬ìš©ì  ì¸¡ë©´ì—ì„œ ë°”ë¼ë³´ë©° í¬ìŠ¤íŒ…í•  ì˜ˆì •ì´ë‹¤. 



## RNN: Long-term dependency

![image](https://user-images.githubusercontent.com/38639633/108290420-b4c85c80-71d3-11eb-8d2d-dcbe3e1a4d69.png)

- "I go home"ì´ë¼ëŠ” ë¬¸ì¥ì„ ë°›ì•˜ì„ ë•Œ ë§¤ time step $t$ë§ˆë‹¤ $x_t, h_{t-1}$ì„ ë°›ì•„ì„œ $h_t$ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤. 
- ê·¸ë¦¼ì—ì„œ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ ë°©í–¥ìœ¼ë¡œ ê°€ë©° ê³„ì‚°ë˜ëŠ” hidden stateë¥¼ encodingí•˜ê²Œ ëœë‹¤. 
- Attention ì—°ì‚°ì„ í•œë‹¤í•´ë„, ë’¤ë¡œ ê°ˆìˆ˜ë¡ ë¨¼ì € ì…ë ¥ëœ ë‹¨ì–´ "I"ëŠ” í¬ì„ë˜ê²Œ ëœë‹¤.  


## Bi-Directional RNNs

![image](https://user-images.githubusercontent.com/38639633/108291960-81d39800-71d6-11eb-945c-96fb9f0bd052.png)

- Vanilla RNNì˜ ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•œ ë°©ì‹ìœ¼ë¡œ ì œì•ˆëœ Bi-directional RNNì€ ì—­ë°©í–¥ìœ¼ë¡œë„ í•œë²ˆ ë” ì§„í–‰í•´ì˜¤ë©´ì„œ ì–‘ë°©í–¥ì—ì„œì˜ encoding ë²¡í„°ë¥¼ í•™ìŠµí•œë‹¤. 
- ì–‘ë°©í–¥ìœ¼ë¡œ ì§„í–‰ë˜ëŠ” Forward RNNê³¼ Backward RNN ëª¨ë“ˆì„ ë³‘ë ¬ì ìœ¼ë¡œ ë§Œë“¤ê³  íŠ¹ì •í•œ timestepì—ì„œì˜ hidden state vectorë¥¼ concatenateí•¨ìœ¼ë¡œì¨ ë‘ ë°°ì˜ ì‚¬ì´ì¦ˆë¡œ ë§Œë“¤ì–´ì§„ encoding vectorë¥¼ ë§Œë“ ë‹¤. 



## Transformer: Long-Term Dependency

![image](https://user-images.githubusercontent.com/38639633/108292428-6b7a0c00-71d7-11eb-80d8-66673d3e3cc7.png){:width="60%"}{:.center}

- Transformerì˜ attention ì—°ì‚°ì€ self-attentionìœ¼ë¡œì¨, ê¸°ì¡´ attentionì—ì„œ encoderì™€ decoderì˜ ì…ë ¥ ë²¡í„°ê°€ ë‹¬ëë˜ ê²ƒê³¼ ë‹¬ë¦¬ ê°™ì€ hidden state vectorë¥¼ ì‚¬ìš©í•œë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. 
- ì¦‰, ê·¸ë¦¼ì—ì„œ $x_1$ì€ decoder hidden state vectorì„ê³¼ ë™ì‹œì— encoder hidden state vector setì¸ $[x_1, x_2, x_3]$ì¤‘ í•˜ë‚˜ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. 
- ê·¸ëŸ¬ë©´ ì²« ë²ˆì§¸ timestepì„ ê¸°ì¤€ìœ¼ë¡œ $x_1$ì€ $[x_1, x_2, x_3]$ ì„¸ encoder hidden states ë“¤ê³¼ ë‚´ì ì„ í†µí•´ attention scoreë¥¼ ê³„ì‚°í•˜ê²Œ ë˜ê³ , ì´ëŠ” $h_1$ìœ¼ë¡œ ê³„ì‚°ë  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. 
- ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ $h_2, h_3$ë¥¼ êµ¬í•˜ê²Œ ë˜ëŠ” í° í‹€ì—ì„œì˜ ë°©ì‹ì„ `Self-Attention`ì´ë¼ê³  ë¶€ë¥¸ë‹¤.
- í•˜ì§€ë§Œ, ì¼ë°˜ì ì¸ ë°©ì‹ìœ¼ë¡œ ê³„ì‚°í•˜ê²Œ ëœë‹¤ë©´ ë‹¹ì—°í•˜ê²Œë„ **ìê¸° ìì‹ ê³¼ì˜ ë‚´ì **ì´ í° ë¹„ì¤‘ìœ¼ë¡œ í• ë‹¹ë˜ê²Œ ë˜ê³ , self-attention moduleì˜ outputì¸ $h_{1,2,3}$ëŠ” ìê¸° ìì‹ ì— ëŒ€í•œ ê°€ì¤‘ í‰ê· ì´ ë†’ê²Œ ì¡íˆê²Œ ë  ê²ƒì´ë‹¤. 
- ë”°ë¼ì„œ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê°œì„ í•˜ê³ ì Transformerì—ì„œëŠ” í™•ì¥ëœ ë°©ì‹ì˜ attention moduleì„ ì‚¬ìš©í•œë‹¤.



### Query, Key, Value Vectors

- **`Query vector`** : encoder-decoder êµ¬ì¡°ì—ì„œ decoder hidden state vectorì— í•´ë‹¹í•˜ëŠ” vectorë¥¼ ì˜ë¯¸í•œë‹¤. ì¦‰, í˜„ì¬ timestep $t$ì—ì„œ ê³„ì‚°í•  ì£¼ì²´ê°€ ë˜ëŠ” vector.
- **`Key vector`** : query vectorì™€ ë‚´ì ì„ í•˜ê²Œ ë  ê°ê°ì˜ ì¬ë£Œ ë²¡í„°ë¥¼ ì˜ë¯¸í•œë‹¤. ì¦‰, encoder-decoder êµ¬ì¡°ì—ì„œ encoder hidden statesì¸ $h_{1,2,3}^{(e)}$ë¥¼ ì˜ë¯¸í•œë‹¤. 
- **`Value vector`** : ê³„ì‚°ëœ ê°€ì¤‘ì¹˜(attention score)ë¥¼ ê°€ì¤‘ í‰ê· í•´ì„œ ê·¸ ë¹„ì¤‘ì„ ê°€ì¤‘í•´ì£¼ê¸° ìœ„í•´ ê³±í•´ì£¼ëŠ” ì›ë˜ ë²¡í„° 

![image](https://user-images.githubusercontent.com/38639633/108337579-9afd3880-7218-11eb-8130-b582c472370e.png)

> - $q_1\cdot k_1$, $q_1\cdot k_2$, $q_1\cdot k_3$ë¥¼ í†µí•´ [3.8, -0.2, 5.9]ì˜ vectorë¥¼ ì–»ê²Œëœë‹¤.   
> - ì´ëŠ” softmaxë¥¼ í†µê³¼í•˜ì—¬ [0.2, 0.1, 0.7]ì´ ëœë‹¤.   
> - ì´ë ‡ê²Œ ë‚˜ì˜¨ ê²°ê³¼ëŠ” [$v_1, v_2, v_3$]ê³¼ pairwise product ì—°ì‚°ì„ ì§„í–‰í•˜ê²Œëœë‹¤.   
> - ê²°ê³¼ì ìœ¼ë¡œ $h_1=0.2v_1+0.1v_2+0.7v_3$ê°€ ëœë‹¤. 

ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ì—°ì‚°ë˜ê¸° ë•Œë¬¸ì—, ìê¸° ìì‹ ì— ëŒ€í•œ self-attention ì—°ì‚°ì„ í•˜ì—¬ë„ ê·¸ í¬ê¸°ê°€ ë†’ì§€ ì•Šê²Œ ëœë‹¤. 

### Operation process in self-attention

![image](https://user-images.githubusercontent.com/38639633/108339298-9afe3800-721a-11eb-90e5-31f24e7d278f.png){:width="80%"}{:.center}

- ì‹¤ì œ ì‘ë™ì€ ìœ„ì™€ ê°™ì€ í–‰ë ¬ ì—°ì‚°ì— ì˜í•´ì„œ ì§„í–‰ëœë‹¤. 
- Embeddingëœ input $X$ëŠ” $W^{Q,K,V}$ì™€ì˜ í–‰ë ¬ê³±ì„ í†µí•´ $Q,K,V$ë¡œ êµ¬ì„±ëœë‹¤. 
- $Q,K,V$ì˜ ê° í–‰ì€ $X$ì˜ ê° í–‰, ì¦‰ ê° í† í°ì— í•´ë‹¹í•˜ëŠ” vectorê°€ ëœë‹¤. 

ì´ ê°™ì€ ë°©ì‹ì„ í†µí•´ ë¨¼ ë‹¨ì–´ê°„ì˜ ê´€ê³„ ë° ìœ ì‚¬ë„ë¥¼ ì´ì „ ëª¨ë¸ê³¼ëŠ” ë‹¬ë¦¬ ì†ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. 



## Transformer: Scaled Dot-Product Attention

- **Inputs** : a query $q$ and a set of key-value $(k, v)$ pairs to an output  
- Query, key, value, and output is all vectors  
- **Output** is weighted sum of values  
- Weight of each value is computed by an inner product of query and corresponding key  
- Queries and keys have same dimensionality $d_k$, and dimensionality of value is $d_v$
	- Value vectorëŠ” ë§ˆì§€ë§‰ì— ê³„ì‚°ëœ ê°€ì¤‘í‰ê· ì„ ê³±í•˜ëŠ” ì—­í• ë§Œì„ í•˜ê¸° ë•Œë¬¸ì— ì°¨ì›ì˜ í¬ê¸°ê°€ Query, Key vectorë“¤ê³¼ëŠ” ë‹¬ë¼ë„ ìƒê´€ì´ ì—†ë‹¤. 

$$
A(q, K, V)=\sum_i\frac{exp(q\cdot k_i)}{\sum_j exp(q\cdot k_j)}v_i
$$

- ì¦‰, inputì€ í•˜ë‚˜ì§œë¦¬ query ë²¡í„° $q$ì™€ $K, V$ê°€ ëœë‹¤.   

- ans it becomes : $A(Q, K, V) = softmax(QK^T)V$.

	![image](https://user-images.githubusercontent.com/38639633/108349953-f1717380-7226-11eb-95b1-544cc34ed8c0.png)

	> ë…¼ë¬¸ì—ì„œì˜ Transformer êµ¬í˜„ ìƒìœ¼ë¡œëŠ” ë™ì¼í•œ shapeìœ¼ë¡œ mappingëœ Q, K, Vê°€ ì‚¬ìš©ë˜ì–´ ê° matrixì˜ shapeì€ ëª¨ë‘ ë™ì¼í•˜ë‹¤. 



### Problem

- As $d_k$ gets large, the variance of $q^Tk$ increases
	- queryì™€ key vectorì˜ ì°¨ì›ì´ ì»¤ì§ˆìˆ˜ë¡ í•´ë‹¹ ë‚´ì ì— ì°¸ì—¬í•˜ëŠ” dimension ì—­ì‹œ ì»¤ì§€ê²Œ ë˜ê³  ì´ë•Œì˜ ë¶„ì‚°ì€ ì ì  ì»¤ì§€ê²Œ ëœë‹¤. 
- Some values inside the softmax get large
- The softmax gets very peaked
- Hence, its gradient gets smaller

### Solution

- Scaled by the length of query / key vectors:
	- $$A(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$
	- $\sqrt{d_k}$ë¡œ ë‚˜ëˆ ì¤Œìœ¼ë¡œì¨ scalingì„ ì‹œì¼œì¤€ë‹¤. 

![image](https://user-images.githubusercontent.com/38639633/108353903-07356780-722c-11eb-9926-69f1500536ac.png){:width="30%"}{:.center}

# Transformer II (contâ€™d)

Transformer(Self-Attention)ì— ëŒ€í•´ ì´ì–´ì„œ ìì„¸íˆ ì•Œì•„ë´…ë‹ˆë‹¤.

**Further Reading**

- [Attention is all you need, NeurIPS'17](https://arxiv.org/abs/1706.03762)
- [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Group Normalization](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf)

**Further Question**

- Attentionì€ ì´ë¦„ ê·¸ëŒ€ë¡œ ì–´ë–¤ ë‹¨ì–´ì˜ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ê°€ì ¸ì˜¬ ì§€ ì•Œë ¤ì£¼ëŠ” ì§ê´€ì ì¸ ë°©ë²•ì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤. Attentionì„ ëª¨ë¸ì˜ Outputì„ ì„¤ëª…í•˜ëŠ” ë°ì— í™œìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?
	- ì°¸ê³ : [Attention is not explanation](https://arxiv.org/pdf/1902.10186.pdf)
	- ì°¸ê³ : [Attention is not not explanation](https://www.aclweb.org/anthology/D19-1002.pdf)

## Transformer: Multi-Head Attention

ì—¬ëŸ¬ ë²„ì „ì˜ headë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ë§Œë“  ë’¤ ì—¬ëŸ¬ë²ˆ ìˆ˜í–‰í•˜ì—¬ ìœ„í—˜ ë¶€ë‹´(?)ì„ ì¤„ì¸ë‹¤.

- The input word vectors are the queries, keys and values
- In other words, the word vectors themselves select each other
- **Problem** of single attention
	- Only one way for words to interact with one another
- **Solution**
	- Multi-head attention maps $ğ‘„, ğ¾, ğ‘‰$ into the $â„$ number of lower-dimensional spaces via $ğ‘Š$ matrices
- Then apply attention, then concatenate outputs and pipe through linear layer

![image](https://user-images.githubusercontent.com/38639633/108366124-58992300-723b-11eb-8c44-0dcb63b16906.png){:width="40%"}{:.center}

---

- Example from illustrated transformer

	1. ê° í—¤ë“œë³„ë¡œ self-attention ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤. 

		![image](https://user-images.githubusercontent.com/38639633/108367812-28eb1a80-723d-11eb-93d9-d2b1952d1c5d.png)

	2. Headë³„ë¡œ ê³„ì‚°ëœ context vectorë¥¼ concatenateí•œë‹¤. 

		![image](https://user-images.githubusercontent.com/38639633/108367991-4f10ba80-723d-11eb-887c-4f7ea326e638.png)

		![image](https://user-images.githubusercontent.com/38639633/108368078-68b20200-723d-11eb-94aa-e903c6060d7c.png)

	3. single attention moduleì˜ ì•„ì›ƒí’‹ê³¼ ë™ì¼í•œ ì‚¬ì´ì¦ˆë¥¼ ìœ„í•´ ì ì ˆí•œ ê°€ì¤‘ì¹˜ matrixë¥¼ ê³±í•´ì¤€ë‹¤.

		![image](https://user-images.githubusercontent.com/38639633/108368310-a6168f80-723d-11eb-9861-31c11216f486.png)

### per-layer complexity

- Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types

	- $n$ is the sequence length
	- $d$ is the dimension of representation
	- $k$ is the kernel size of convolutions
	- $r$ is the size of the neighborhood in restricted self-attention

	![image](https://user-images.githubusercontent.com/38639633/108372541-479fe000-7242-11eb-9046-97af2ebdd5ca.png)

	

## Transformer: Block-Based Model

ê° Blockì€ ë‘ ê°œì˜ sub-layersë¥¼ ì§€ë‹Œë‹¤. 

- Multi-head attention ëª¨ë“ˆ
- Two-layer feed-forward NN(with ReLU)

ê·¸ë¦¬ê³  ì´ ë‘ ê°œì˜ ëª¨ë“ˆì€ ëª¨ë‘ Residual connectionê³¼ layer normalization ìŠ¤í…ì„ ê±°ì¹œë‹¤

- $LayerNorm(x+sublayer(x))$



### Residual connection

![image](https://user-images.githubusercontent.com/38639633/108375943-cba79700-7245-11eb-851f-4b3cf7b8d32a.png){:width="30%"}{:.center}

- residual connectionì€ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ ì…ë ¥ ë²¡í„°ë¥¼ attention layerë¥¼ í†µê³¼í•œ outputì— ë‹¤ì‹œ ë”í•´ì£¼ëŠ” ë°©ì‹ì´ë‹¤
- ì´ë¥¼ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ” íš¨ê³¼ëŠ” **ì˜¨ì „íˆ** input vectorê°€ attention layerë¥¼ í†µê³¼í•œ ë’¤ì˜ ê²°ê³¼ë§Œì„ ë°˜ì˜í•œë‹¤ëŠ” ì ì´ë‹¤. 
	- ì˜ˆë¥¼ë“¤ì–´ input vector [1, -4]ê°€ ì¼ë°˜ì ì¸ attention moduleì„ í†µê³¼í•œ ë’¤ [2, 3]ì´ ë˜ì—ˆë‹¤ê³  ê°€ì •í•˜ë©´, **residual connection**ì„ í†µí•´ [3, -1]ì˜ ë²¡í„°ë¥¼ ë§Œë“¤ê²Œí•œë‹¤. ì´ëŠ” í•™ìŠµê³¼ì •ì—ì„œ ì˜¨ì „íˆ attention moduleì˜ ì—­í• ì´ [2, 3]ì˜ ë²¡í„°ë¥¼ ë§Œë“¤ê²Œë” ìœ ë„í•˜ëŠ” ì—­í• ì„ í•œë‹¤. 
- ì´ ê³¼ì •ì„ í†µí•´ gradient vanishing ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , í•™ìŠµì´ ì•ˆì •ë  ìˆ˜ ìˆë„ë¡ í•œë‹¤. 



### Layer Normalization

í•™ìŠµ ë„ì¤‘ ìƒ˜í”Œì˜ ë¶„í¬ë¥¼ normalization í•´ì£¼ëŠ” ë‹¤ì–‘í•œ ë°©ì‹ì´ ì¡´ì¬í•œë‹¤. 

Layer normalization changes input to have zero mean and unit variance, per layer and per training point (and adds two more parameters)

![image](https://user-images.githubusercontent.com/38639633/108378501-6608da00-7248-11eb-8f99-2f86642b0009.png)

![image](https://user-images.githubusercontent.com/38639633/108378523-6acd8e00-7248-11eb-829a-ee974d91989e.png)

ì°¨ì´ëŠ” ìˆì§€ë§Œ, ê° ìƒ˜í”Œë“¤ì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ê³¼ì •ì´ë‹¤. ì´ ê³¼ì •ì€ Neural networkì˜ íŠ¹ì • nodeì—ì„œ ì›í•˜ëŠ” ë§Œí¼ì˜ ê°’ì„ ê°€ì§€ë„ë¡ ì¡°ì ˆí•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤. 

ì–´ì¨Œë“  ë‹¤ì–‘í•œ normalizationì¤‘ ìš°ë¦¬ê°€ ë³¼ Layer normalizationì€ ë‘ ê°€ì§€ stepìœ¼ë¡œ êµ¬ì„±ëœë‹¤.

- Normalization of each word vectors to have mean of zero and variance of one.

- Affine transformation of each sequence vector with learnable parameters

	![image](https://user-images.githubusercontent.com/38639633/108379252-1840a180-7249-11eb-81b8-e45f726690b3.png)

	- thinkingê³¼ machinesë¼ëŠ” ë‹¨ì–´ê°€ ê°ê° 4ì°¨ì›ì˜ vectorë¡œ í‘œí˜„ ë˜ì—ˆì„ ë•Œ, wordë³„ë¡œ íŠ¹ì • layerì—ì„œ ë°œê²¬ë˜ëŠ” 4ê°œì˜ nodeì˜ ê°’ë“¤ì„ ëª¨ì•„ì„œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ëª¨ì•„ì„œ ê°ê° í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ 0ê³¼ 1ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.  
	- ê·¸ëŸ¬ë©´ ì´ í‘œì¤€í™” ì‘ì—…ì„ ê±°ì¹œ vectorì˜ ê°’ë“¤ì€ ë°”ë€Œê²Œ ëœë‹¤.(ê·¸ë¦¼ì—ì„œ `2ë²ˆì§¸`)
	- ì´ë ‡ê²Œ ë³€í™˜ëœ vectorë¥¼ `Affine transformation`í•˜ì—¬ ê²°ê³¼ë¡œ ë„ì¶œë˜ê²Œ ëœë‹¤. 

**ì´ë ‡ë“¯ layer normalizationì„ ê±°ì¹˜ë©´ì„œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” í‰ê· ê³¼ ë¶„ì‚°ì„ ì£¼ì…í•  ìˆ˜ ìˆê²Œ ëœë‹¤.** 



## Transformer: Positional Encoding

RNN ê³„ì—´ì˜ ëª¨ë¸ì€ time stepì— ë”°ë¥¸ inputì˜ ìˆœì„œê°€ ìì—°ìŠ¤ë ˆ ì •í•´ì§€ê²Œ ëœë‹¤. í•˜ì§€ë§Œ, token(word)ì˜ ìƒëŒ€ì  ìˆœì„œë¥¼ ì•Œë ¤ì£¼ëŠ” êµ¬ì¡°ê°€ ì—†ëŠ” transformerì—ì„œëŠ” ì´ë¥¼ ìœ„í•œ êµ¬ì¡°ê°€ í•„ìš”í•˜ê¸°ì— ì‚¬ìš©ëœ ë°©ë²•ì´ positional encodingì´ë‹¤. ì´ ë‚´ìš©ì€ ë³¸ ë¸”ë¡œê·¸ì˜ ì´ì „ [í¬ìŠ¤íŒ…](https://ydy8989.github.io/2021-01-10-transformer/#23-positional-encoding)ì— ì‘ì„±í•˜ì˜€ê¸°ì— ìƒëµí•˜ë„ë¡ í•œë‹¤. 

![image](https://user-images.githubusercontent.com/38639633/108387657-22ff3480-7251-11eb-9ef8-eca0ba054505.png)



## Transformer: Warm-up Learning Rate Scheduler

í•™ìŠµì„ ì¡°ê¸ˆ ë” ë¹ ë¥´ê²Œ í•˜ê³ , ìµœì¢… ìˆ˜ë ´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ë°©ì‹ìœ¼ë¡œì¨ ê³ ì •ëœ learning rateë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ì•„ë‹ˆë¼ í•™ìŠµì´ ì§„í–‰ë¨ì— ë”°ë¼ lrì„ ë³€ê²½ì‹œí‚¤ëŠ” ë°©ì‹ì´ë‹¤. 

- Learning rate = $$d_{model}^{âˆ’0.5}\cdot min(\#step^{-0.5}, \#step\cdot warmup\_stps^{-1.5})$$ 

	![image](https://user-images.githubusercontent.com/38639633/108388380-eaac2600-7251-11eb-81e8-5edf7176a741.png)



## Transformer: Encoder Self-Attention Visualization

- Words start to pay attention to other words in sensible ways

	![image](https://user-images.githubusercontent.com/38639633/108390240-cfdab100-7253-11eb-9da1-4a4da3950585.png)

	> [Colab link](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)ì— ê°€ë©´ visualizationì— ëŒ€í•œ tutorialì„ ì‹œë„í•´ë³¼ ìˆ˜ ìˆë‹¤.
	>
	> - ì¢Œì¸¡ìƒë‹¨ "ë“œë¼ì´ë¸Œë¡œ ë³µì‚¬" í´ë¦­ í›„ ê¸°ì¡´ì°½ ë‹«ê³  ìƒˆë¡œ ì—´ë¦°ì°½ì—ì„œ ì‹¤í–‰ 

	

## Transformer: Decoder

- Two sub-layer changes in decoder

- Masked decoder self-attention on previously generated outputs:

	![image](https://user-images.githubusercontent.com/38639633/108591796-d67d3b80-73ad-11eb-8113-9b57564723d4.png)

- Encoder-Decoder attention, where queries come from previous decoder layer and keys and values come from output of encoder

	![image](https://user-images.githubusercontent.com/38639633/108591809-e39a2a80-73ad-11eb-8b93-85eecc029198.png)

Inputê³¼ ë¹„ìŠ·í•˜ê²Œ, Outputì„ shifted rightí•˜ì—¬ ì…ë ¥í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ë³´ì.

- Input : 'I go home'
- Output : '\<SOS> ë‚˜ëŠ” ì§‘ì—' ë¡œ ì…ë ¥ sequenceë¡œ ì£¼ì–´ì§„ë‹¤. 
	- ground truth : 'ë‚˜ëŠ” ì§‘ì— ê°„ë‹¤'

ì´ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ shiftingëœ outputì„ ì…ë ¥ìœ¼ë¡œ ì¤€ë‹¤.(seq2seqì—ì„œ decoder inputê³¼ ê°™ì€ì—­í• )



Transformerì˜ Decoderì—ëŠ” ì´ 2ê°œì˜ attention moduleì´ ìˆë‹¤. 

- Masked Multi-head attention
- Multi-head attention

### Multi-head attention(Decoder)

![image](https://user-images.githubusercontent.com/38639633/108594746-1b5d9e00-73bf-11eb-8486-283649b71c21.png)

ìœ„ ê·¸ë¦¼ì˜ ë¹¨ê°„ìƒ‰ ì‚¬ê°í˜• ë¶€ë¶„ì˜ Multi-Head Attention êµ¬ì¡°ëŠ” Encoderì˜ multi-head attention moduleê³¼ êµ¬ì¡°ìƒìœ¼ë¡œëŠ” ë™ì¼í•˜ë‹¤. ë‹¨, ì°¨ì´ì ì´ ìˆë‹¤ë©´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” Q, K, V ë²¡í„°ê°€ ë‹¤ë¥¸ë°, ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ `ë‘ ê°œ`ì˜ í™”ì‚´í‘œëŠ” encoderë¡œë¶€í„° ë“¤ì–´ì˜¤ê³ , `í•œ ê°œ`ì˜ í™”ì‚´í‘œëŠ” ì•„ë˜ Masked multi-head attention moduleë¡œë¶€í„° ì˜¨ë‹¤. 

`Query` ë²¡í„°ëŠ” Masked multi-head attentionìœ¼ë¡œë¶€í„° ì˜¤ê³ , `Key` ë²¡í„°ì™€ `Value` ë²¡í„°ëŠ” í•™ìŠµëœ ìƒíƒœë¡œ Encoderì—ì„œ ë“¤ì–´ì˜¨ë‹¤. 

íŠ¹íˆ, Masked multi-head attentionìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ Residual connectionì€ decoderì˜ inputìœ¼ë¡œë¶€í„° ì˜¨ queryê°’ê³¼ encoderì—ì„œ ë„˜ì–´ì˜¨ ë²¡í„°ë¥¼ ê²°í•©í•˜ê²Œ í•´ì£¼ëŠ” ì—­í• ì„ í•  ê²ƒì´ë‹¤. 

ìµœì¢…ì ìœ¼ë¡œ ê° ë²¡í„°ì˜ ì¶œë ¥ì€ FFNì™€ softmax layerë¥¼ ê±°ì¹œ ë’¤ predict ë˜ê³ , ì´ëŠ” ground truthì™€ ë¹„êµí•˜ì—¬ back prop.ì„ ê³„ì‚°í•˜ê²Œ ëœë‹¤. 



### Masked Multi-Head attention(Decoder)

- Those words not yet generated cannot be accessed during the inference time
- Renormalization of softmax output prevents the model from accessing ungenerated words

Decoderì˜ ì²«ë²ˆì§¸ inputì´ ë“¤ì–´ì˜¨ ë’¤ì˜ multi-head attention layerë¡œì¨, ì¶œë ¥ ë‹¨ì–´ê°€ ìê¸°ë³´ë‹¤ ì•ì„œ ì´ë¯¸ ì•ì— ë‚˜ì™”ë˜ ë‹¨ì–´ë“¤ë§Œ ì°¸ê³ í•´ì„œ ì—°ì‚°í•˜ëŠ” attention layerì´ë‹¤. ë’¤ìª½ì— ë‚˜ì˜¨ ë‹¨ì–´ê¹Œì§€ ì°¸ê³ í•´ ì•ì„ ì˜ˆì¸¡í•˜ê²Œ í•œë‹¤ë©´ ì´ëŠ” ì¼ì¢…ì˜ cheatingì²˜ëŸ¼ ì‘ìš©í•´ auto-regressiveë¥¼ ìˆ˜í–‰í•˜ì§€ ëª»í•˜ëŠ” ëª¨ë¸ì´ ëœë‹¤.

ì´ë¥¼ ìœ„í•´ì„œëŠ” í˜„ì¬ ì§„í–‰ì¤‘ì¸ ë‹¨ì–´ë³´ë‹¤ ë’¤ìª½ ë‹¨ì–´ë“¤ì— ëŒ€í•œ masking ì‘ì—…ì´ í•„ìš”í•˜ë‹¤. Maskingì˜ ë°©ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.

| query/key |  I   |    am    |    a     |   boy    |
| :-------: | :--: | :------: | :------: | :------: |
|     I     |  23  | $\infty$ | $\infty$ | $\infty$ |
|    am     |  15  |    27    | $\infty$ | $\infty$ |
|     a     |  14  |    20    |    23    | $\infty$ |
|    boy    |  11  |    18    |    22    |    25    |

$Q\times K^T$ë¥¼ í†µí•´ ë§Œë“¤ì–´ì§„ í–‰ë ¬ì„ ìœ„ì™€ ê°™ì´ ì°¸ê³ í•˜ì§€ ì•Šì„ ë¶€ë¶„ì„ $\infty$ë¡œ í• ë‹¹í•´ ì¤Œìœ¼ë¡œì¨ softmaxì˜ outputì´ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ê²Œ ë§Œë“ ë‹¤.

![masked](../../assets/img/boostcamp/masked.gif){:width="50%"}![masked2](../../assets/img/boostcamp/masked2.gif){:width="50%"}



## Transformer: Experimental Results

- Results on English-German/French translation (newstest2014)

![image](https://user-images.githubusercontent.com/38639633/108597619-4f3fc000-73cd-11eb-811d-f46cdff6226f.png)

BLEU ìŠ¤ì½”ì–´ê°€ 50%ê°€ ì•ˆë˜ëŠ” ì„±ëŠ¥ìœ¼ë¡œ ë³´ì´ë”ë¼ë„, ìš°ë¦¬ë‚˜ë¼ ë§ì²˜ëŸ¼ ì–´ìˆœì˜ ë³€í™”ê°€ ìˆì§€ë§Œ ì´í•´í•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ì—†ëŠ” ê²½ìš°ë„ ë§ê¸°ì— ë‚˜ì˜ì§€ ì•Šì€ ì„±ëŠ¥ìœ¼ë¡œ ì·¨ê¸‰ëœë‹¤. 

