# (1ê°•) Intro to NLP, Bag-of-Words

ìì—°ì–´ ì²˜ë¦¬ì˜ ì²« ì‹œê°„ìœ¼ë¡œ NLPì— ëŒ€í•´ ì§§ê²Œ ì†Œê°œí•˜ê³  ìì—°ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì¸ **Bag-of-Words**ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. Bag-of-WordsëŠ” ë‹¨ì–´ì˜ í‘œí˜„ì— ìˆì–´ì„œ one-hot-encodingì„ ì´ìš©í•˜ë©°, ë‹¨ì–´ì˜ ë“±ì¥ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ” ì•„ì£¼ ê°„ë‹¨í•œ ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ê°„ë‹¨í•œ ëª¨ë¸ì´ì§€ë§Œ ë§ì€ ìì—°ì–´ ì²˜ë¦¬ taskì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³ , ì´ Bag-of-Wordsë¥¼ ì´ìš©í•´ ë¬¸ì„œë¥¼ ë¶„ë¥˜í•˜ëŠ” **Naive Bayes Classifier**ì— ëŒ€í•´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” **ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•, ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•**ì— ëŒ€í•´ ê³ ë¯¼í•´ë³´ë©´ì„œ ê°•ì˜ë¥¼ ë“¤ì–´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.



## Intro to Natural Language Processing(NLP)

### ì´ë²ˆ ê³¼ì •ì˜ ëª©í‘œ

- Natural language processing (NLP), which aims at properly understanding and generating
	human languages(NLG), emerges as a crucial application of artificial intelligence, with the
	advancements of deep neural networks.
- This course will cover various deep learning approaches as well as their applications such as
	language modeling, **machine translation, question answering, document classification, and dialog systems**. 



###  í•™ë¬¸ì  ì²´ê³„

- NLP(ì£¼ìš” í•™íšŒ : ACL, EMNLP, NAACL)
	- low level parsing
		- í† í¬ë‚˜ì´ì§•, stemming(ì–´ë¯¸ì˜ ë³€í™”ì— ëŒ€í•œ ì—°êµ¬, ì–´ê·¼ ì¶”ì¶œ)
	- word and phrase level
		- ê°œì²´ëª… ì¸ì‹(NER) : ë‹¨ì¼ ë‹¨ì–´ í˜¹ì€ ì—¬ëŸ¬ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ê³ ìœ ëª…ì‚¬ë¥¼ ì¸ì‹í•˜ëŠ” ì‘ì—…
		- POS(part-of-speech) tagging : ë¬¸ì¥ë‚´ì—ì„œ ì›Œë“œì˜ í’ˆì‚¬ë‚˜ ì„±ë¶„ì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë‚´ëŠ” task
		- noun-phrase chunking 
		- dependecy íŒŒì‹±
		- coreference resolution 
	- Sentence level
		- ê°ì •ë¶„ì„(sentiment analysis)
		- machine translation
	- Multi-sentence and paragraph level
		- Entailment prediction : ë‘ ë¬¸ì¥ê°„ì˜ ë…¼ë¦¬ì  ë‚´í¬ ë° ëª¨ìˆœê´€ê³„ ì¶”ë¡ 
		- question answering  : ë…í•´ ê¸°ë°˜ì˜ ì§ˆì˜ì‘ë‹µ
		- dialog systems : ëŒ€í™”ëª¨ë¸, ì±—ë´‡ì„.
		- summarization
- Text mining(KDD, The webconf(formerly, WWW), WSDM, CIKM, IWSM)
	- ë¹…ë°ì´í„°ì™€ ì—°ê´€ëœ ê²½ìš°ê°€ ë§ë‹¤. 
	- Extract useful information and insights from text and document data
	- Document clustering (e.g., topic modeling)
	- Highly related to computational social science - íŠ¸ìœ„í„°ë‚˜ ì†Œì…œ ë¯¸ë””ì–´ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬íšŒí˜„ìƒ ë“±ë“±ì„ ë¶„ì„í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤. 
- Information retrieval - ì •ë³´ê²€ìƒ‰ ë¶„ì•¼ (ì£¼ìš” í•™íšŒ : SIGIR, WSDM, CIKM, RecSys)
	- Highly related to computational social science
		- ì´ë¯¸ ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ê³ ë„í™” ë˜ì–´ ìƒëŒ€ì ìœ¼ë¡œ ì—°êµ¬ê°€ ë”ë”˜ ë¶„ì•¼ì´ë‹¤. 
		- í•˜ì§€ë§Œ, ì¶”ì²œì‹œìŠ¤í…œ ë¶„ì•¼ëŠ” í™œë°œíˆ ì—°êµ¬ë˜ê³  ìˆë‹¤. 



### NLP ë¶„ì•¼ì˜ íŠ¸ë Œë“œ

- ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë‚˜íƒ€ë‚´ëŠ” í…Œí¬ë‹‰ w2v or glove
- RNN ê³„ì—´ì˜ ëª¨ë¸
- attention moduleì— ê¸°ë°˜í•œ transformer ëª¨ë¸
- ê° NLP taskì— ë§ëŠ” ì„¸ë¶€ì ì¸ ëª¨ë¸ ì„¤ê³„ë¡œ ë¶„í™”ë¨
- self-supervised í•™ìŠµëœ ëª¨ë¸ë“¤(bert, gpt-3)ì€ íŠ¹ì • íƒœìŠ¤í¬ì—ì„œ ë²—ì–´ë‚˜ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ë¡œ ë°œì „í•˜ì˜€ë‹¤. 
- ëª¨ë¸ì´ ì»¤ì§ìœ¼ë¡œ ì¸í•œ ìì›ì  í•œê³„ë¡œ ì „ì´í•™ìŠµì´ íŠ¸ë Œë“œê°€ë¨



## Bag of Words

### Bag-of-Words Representation

- Step 1. ìœ ë‹ˆí¬í•œ ë‹¨ì–´ë¥¼ ëª¨ì•„ì„œ vocabì„ êµ¬ì¶•í•œë‹¤.

	-  Example sentences: â€œJohn really really loves this movieâ€œ, â€œJane really likes this songâ€
	-  Vocabulary: {â€œJohnâ€œ, â€œreallyâ€œ, â€œlovesâ€œ, â€œthisâ€œ, â€œmovieâ€œ, â€œJaneâ€œ, â€œlikesâ€œ, â€œsongâ€}

- Step 2. ê° ë‹¨ì–´ë¥¼ one-hot ë²¡í„°ë¡œ í‘œí˜„í•œë‹¤.

	- Vocabulary: {â€œJohnâ€œ, â€œreallyâ€œ, â€œlovesâ€œ, â€œthisâ€œ, â€œmovieâ€œ, â€œJaneâ€œ, â€œlikesâ€œ, â€œsongâ€}

		![image](https://user-images.githubusercontent.com/38639633/107896991-49844d80-6f7b-11eb-9c7e-ef232de8ef90.png)

	- ì„ì˜ì˜ ë‘ ë‹¨ì–´ìŒì˜ **ìœ í´ë¦¬ë””ì–¸ distance**ëŠ” $\sqrt 2$ ì´ê³ , **ì½”ì‚¬ì¸ ìœ ì‚¬ë„**ëŠ” ëª¨ë‘ 0ì´ë‹¤ 

	- ì¦‰, ë‹¨ì–´ì˜ ì˜ë¯¸ì™€ ìƒê´€ì—†ì´ ëª¨ë‘ ë™ì¼í•˜ê²Œ ì„¤ì •ëœë‹¤. 

- ë¬¸ì¥/ë¬¸ì„œëŠ” ì´ëŸ¬í•œ one-hot ë²¡í„°ë“¤ì˜ í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. 

	![image](https://user-images.githubusercontent.com/38639633/107897153-c6afc280-6f7b-11eb-9631-79ab2cd67ac6.png)



### NaiveBayes Classifier for Document Classification

ìœ„ì™€ ê°™ì´ Bag of Words ë²¡í„°ë¡œ ë‚˜íƒ€ë‚¸ ë¬¸ì„œë¥¼ ì •í•´ì§„ ì¹´í…Œê³ ë¦¬ í˜¹ì€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œì ì¸ ë°©ë²•ì¸ NaiveBayes Classifierë¥¼ ì•Œì•„ë³´ì

![image](https://user-images.githubusercontent.com/38639633/107897302-10001200-6f7c-11eb-97a8-eca67706a481.png){:width="80%"}{: .center}

- Bayesâ€™ Rule Applied to Documents and Classes

	- For a document d and a class c

		![image](https://user-images.githubusercontent.com/38639633/107927387-4d839000-6fba-11eb-90be-6443b810a5d9.png)

	- For a document `d`, which consists of a sequence of words `w`, and a class `c`

	- The probability of a document can be represented by multiplying the probability of each word appearing

	- $P(d\vert c)P(c)=P(w_1, w_2,\dots,w_n\vert c)P(c)\rightarrow P(c)\prod_{w_i\in W}P(w_i\vert c)$ 

	- íŠ¹ì • ì¹´í…Œê³ ë¦¬ cê°€ ê³ ì •ë˜ì—ˆì„ ë•Œ, ë¬¸ì„œ dê°€ ë‚˜íƒ€ë‚  í™•ë¥ ì´ê³ , ì´ëŠ” $w_1$ë¶€í„° $w_n$ê¹Œì§€ ë™ì‹œì— ë‚˜íƒ€ë‚  ë™ì‹œì‚¬ê±´ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ê° ë‹¨ì–´ê°€ ë“±ì¥í•  í™•ë¥ ì´ ì„œë¡œ ë…ë¦½ì´ë©´, ì´ë¥¼ ê³±í•œ í˜•íƒœë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. 

- Example

	![image](https://user-images.githubusercontent.com/38639633/107929544-2084ac80-6fbd-11eb-9ff9-e79e11777934.png)

	ì´ëŸ¬í•œ ìƒí™©ì—ì„œ Test ë°ì´í„°ì˜ *Classification task usses transformer*ë¼ëŠ” ë¬¸ì¥ì˜ ê° ë‹¨ì–´ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

	![image](https://user-images.githubusercontent.com/38639633/107929512-1662ae00-6fbd-11eb-9f1d-0712dac98a25.png)

	For a test document $ğ‘‘_5$ = â€œClassification task uses transformerâ€

	-  We calculate the conditional probability of the document for each class
	-  We can choose a class that has the highest probability for the document

- $P(C_{cv}\vert d_5)=P(C_{CV})\prod_{w\in W}P(w\vert c_{CV})=\frac{1}{2}\times\frac{1}{10}\times\frac{1}{10}\times\frac{1}{10}\times\frac{1}{10}=0.00005$ ì´ë‹¤.

