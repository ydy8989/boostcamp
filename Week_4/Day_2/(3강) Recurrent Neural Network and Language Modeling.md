# (3ê°•) Recurrent Neural Network and Language Modeling

ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ **Recurrent Neural Network(RNN)**ë¥¼ í™œìš©í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ê³¼ ì´ë¥¼ ì´ìš©í•œ **Language Model**ì„ í•™ìŠµí•©ë‹ˆë‹¤. RNNì€ ë‹¨ì–´ê°„ ìˆœì„œë¥¼ ê°€ì§„ ë¬¸ì¥ì„ í‘œí˜„í•˜ê¸° ìœ„í•´ ìì£¼ ì‚¬ìš©ë˜ì–´ ì™”ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ RNN êµ¬ì¡°ë¥¼ í™œìš©í•´ ë‹¤ì–‘í•œ NLP ë¬¸ì œë¥¼ ì •ì˜í•˜ê³  í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. Language Modelì€ ì´ì „ì— ë“±ì¥í•œ ë‹¨ì–´ë¥¼ conditionìœ¼ë¡œ ë‹¤ìŒì— ë“±ì¥í•  ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì´ì „ì— ë“±ì¥í•œ ë‹¨ì–´ëŠ” ì´ì „ì— í•™ìŠµí–ˆë˜ ë‹¤ì–‘í•œ neural network ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•´ í‘œí˜„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë²ˆ ì‹œê°„ì—ëŠ” RNNì„ ì´ìš©í•œ character-levelì˜ language modelì— ëŒ€í•´ì„œ ì•Œì•„ë´…ë‹ˆë‹¤.

RNNì„ ì´ìš©í•œ Language Modelì—ì„œ ìƒê¸¸ ìˆ˜ ìˆëŠ” ì´ˆë°˜ time stepì˜ ì •ë³´ë¥¼ ì „ë‹¬í•˜ê¸° ì–´ë ¤ìš´ ì , gradient vanishing/explodingì„ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²• ë“±ì— ëŒ€í•´ ë‹¤ì‹œ í•œë²ˆ ë³µìŠµí•  ìˆ˜ ìˆëŠ” ì‹œê°„ì´ ëìœ¼ë©´ í•©ë‹ˆë‹¤.

**Further Reading**

- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [CS231n(2017)_Lecture10_RNN](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)

-----

## Vanilla RNN and Type of RNN

RNNì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ RNN ê³„ì—´ ì‹ ê²½ë§ì˜ ì¢…ë¥˜ì— ê´€í•œ ë‚´ìš©ì€ ì•ì„œ ê³µë¶€í–ˆë˜ ë‚´ìš©ê³¼ ì¤‘ë³µë˜ê¸°ì— ìƒëµí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. 

> ê´€ë ¨ ë‚´ìš©ì— ëŒ€í•œ [í¬ìŠ¤íŒ…](https://ydy8989.github.io/2021-02-04-rnn/)ì—ì„œ í™•ì¸í•˜ê¸°.



## Character-level Language Model

ì–¸ì–´ëª¨ë¸ : ë¬¸ìì—´ì´ë‚˜ ë‹¨ì–´ë“¤ì˜ ìˆœì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë§ì¶”ëŠ” taskë¥¼ ë§í•œë‹¤. 

- Example of training sequence "hello"

	- Vocabulary : [h, e, l, o]

	- Example training sequence: â€œhelloâ€

	- $â„_t = tanh(ğ‘Š_th_{ğ‘¡âˆ’1} + ğ‘Š_{xh}x_t + ğ‘)$ ë¥¼ í†µí•´ì„œ Hidden layer ê³„ì‚°í•œë‹¤

	- output layerì—ì„œ $logit=W_{hy}h_t+b$ë¡œ ê³„ì‚°ëœë‹¤. 

	- logitì´ë¼ê³  í‘œí˜„í•œ ì´ìœ ëŠ”, ë‹¤ìŒ ë‹¨ì–´ í˜¹ì€ characterë¡œ ì˜ˆì¸¡í•´ì•¼í•˜ëŠ” 

	- ê° time stepì—ì„œ ì„ í˜• ë³€í™˜ì„ í†µí•´ ê³„ì‚°ëœ output layerëŠ” softmaxë¥¼ í†µê³¼í•´ outputì„ ë„ì¶œí•˜ê²Œ ëœë‹¤. 

	- ì²«ë²ˆì§¸ timestepì„ ì˜ˆë¡œ ë“¤ë©´, output layerì˜ outputì¸ [1.0, 2.2, -3.0, 4.1]ì€ 'e'ì— í•´ë‹¹í•˜ëŠ” [0, 1, 0, 0]ì— fittingë˜ë„ë¡ í•™ìŠµí•œë‹¤. 

		![image](https://user-images.githubusercontent.com/38639633/108020955-e10f9c00-7060-11eb-864e-f63e5405dcc1.png)



### BPTT(Backpropagation through time)

![image](https://user-images.githubusercontent.com/38639633/108021388-cbe73d00-7061-11eb-85c0-4021e2c142c5.png)

- Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient

- ìœ„ì™€ ê°™ì´ lossë¥¼ ê³„ì‚°í•  ë•Œ í•œë²ˆì— ê³„ì‚°í•œë‹¤ë©´, ë§ì€ ë¦¬ì†ŒìŠ¤ ì œí•œì´ ê±¸ë¦¬ê¸° ë•Œë¬¸ì— **truncated bptt**ê¸°ë²•ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. 

- Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps.

  ![image](https://user-images.githubusercontent.com/38639633/108021514-1072d880-7062-11eb-9104-c44e94952479.png)



# Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)

ì´ ë¶€ë¶„ì— ëŒ€í•œ ë‚´ìš© ì—­ì‹œ ì•ì„  [í¬ìŠ¤íŒ…](https://ydy8989.github.io/2021-02-04-rnn/)ê³¼ ì¤‘ë³µë˜ë¯€ë¡œ ìƒëµí•œë‹¤.

## LSTM

![image](https://user-images.githubusercontent.com/38639633/108021886-cc340800-7062-11eb-9ee8-9d2f41fe8374.png)

- Long short-term memory
  - i: Input gate, Whether to write to cell
  - f: Forget gate, Whether to erase cell
  - o: Output gate, How much to reveal cell
  - g: Gate gate, How much to write to cell
- ìœ„ ê·¸ë¦¼ì˜ ì˜¤ë¥¸ìª½ íŒŒë€ìƒ‰ ë°•ìŠ¤ë¡œ ì´í•´í•˜ì. 

## GRU

- Uninterrupted gradient flow!

	![image](https://user-images.githubusercontent.com/38639633/108022303-a6f3c980-7063-11eb-8edc-4026c482b8f4.png)

- `+`ì—°ì‚°ì€ gradientë¥¼ ë³µì‚¬í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. 

- ì˜¤ë¦¬ì§€ë‚  RNNì— ë¹„í•´ Gradientë¥¼ ì¡°ê¸ˆ ë” ì˜¤ë˜ ë³´ì¡´í•  ìˆ˜ ìˆë‹¤. 



