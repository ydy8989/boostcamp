# (6ê°•) Beam Search and BLEU

**ê°•ì˜ ì†Œê°œ**

ë¬¸ì¥ì„ decoding í•˜ëŠ” ë°ì— ì‚¬ìš©í•˜ëŠ” ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ì¸ **Beam Search**ì™€ ë²ˆì—­ taskì—ì„œ ë²ˆì—­ëœ ë¬¸ì¥ì„ í‰ê°€í•˜ëŠ” ëŒ€í‘œì ì¸ metricì¸ **BLEU score**ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.

ì–¸ì–´ ëª¨ë¸ì´ ë¬¸ì¥ì„ generationí•  ë•Œì—ëŠ” í™•ë¥ ê°’ì— ê¸°ë°˜í•œ ë‹¤ì–‘í•œ ê²½ìš°ì˜ ìˆ˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤í•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì´ë©° ë„ˆë¬´ ì‘ì€ í™•ë¥ ê°’ê¹Œì§€ ê³ ë ¤í•œë‹¤ë©´ ìƒì„±ëœ ë¬¸ì¥ì˜ qualityê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ì¥ ë†’ì€ í™•ë¥ ê°’ì„ ê³ ë ¤í•˜ëŠ” ë°©ë²• ì—­ì‹œ ëª¨ë¸ì´ ë‹¨ìˆœí•œ generationì„ í•˜ë„ë¡ í•˜ê²Œ ë§Œë“œëŠ” ë‹¨ì ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œì˜ ëŒ€ì•ˆìœ¼ë¡œ ì œì•ˆëœ Beam Searchë¥¼ ì•Œì•„ë´…ë‹ˆë‹¤.

ìì—°ì–´ëŠ” ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ë°©ì‹ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ëª¨ë¸ì˜ ì…ë ¥ ë° ì¶œë ¥ìœ¼ë¡œ í™œìš©ë˜ê¸° ë•Œë¬¸ì— ì ì ˆí•œ metricì„ ì´ìš©í•´ ëª¨ë¸ì„ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ìì—°ì–´ì²˜ë¦¬ ê´€ë ¨ metricì´ ìˆì§€ë§Œ, ê·¸ì¤‘ì—ì„œë„ ë²ˆì—­ taskì—ì„œ ê°€ì¥ ëŒ€í‘œì ì¸ BLEU scoreë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ë²ˆì—­ì— ìˆì–´ì„œ BLEU scoreê°€ precisionì„ ê³ ë ¤í•˜ëŠ” ì´ìœ ì— ëŒ€í•´ì„œ ê³ ë¯¼í•˜ë©´ì„œ ê°•ì˜ë¥¼ ë“¤ì–´ì£¼ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

**Further Reading**

- [Deep learning.ai-BeamSearch](https://www.youtube.com/watch?v=RLWuzLLSIgw&feature=youtu.be)
- [Deep learning.ai-RefiningBeamSearch](https://www.youtube.com/watch?v=gb__z7LlN_4&feature=youtu.be)
- [OpenNMT-beam search](https://opennmt.net/OpenNMT/translation/beam_search/)

**Further Question**

- BLEU scoreê°€ ë²ˆì—­ ë¬¸ì¥ í‰ê°€ì— ìˆì–´ì„œ ê°–ëŠ” ë‹¨ì ì€ ë¬´ì—‡ì´ ìˆì„ê¹Œìš”?
	- ì°¸ê³ : [Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics](https://arxiv.org/abs/2006.06264?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+arxiv%2FQSXk+%28ExcitingAds%21+cs+updates+on+arXiv.org%29)

---

## Beam search

ìì—°ì–´ ìƒì„±ëª¨ë¸ì—ì„œ í…ŒìŠ¤íŠ¸ íƒ€ì„ì‹œ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚´ê¸° ìœ„í•œ ë°©ë²•ì¸ `beam search`ë¥¼ ì•Œì•„ë³´ì. 

### Greedy approach

í…ìŠ¤íŠ¸ ìƒì„±ì‹œ ëª¨ë¸ì€ ê°€ì¥ ë†’ì€ í™•ë¥ ë¡œ ë“±ì¥í•  ë‹¨ì–´ë¥¼ ì¶œë ¥í•˜ê²Œ ëœë‹¤. ì´ ë°©ë²•ì€ ì „ì²´ì ì¸ êµ¬ì¡°ì—ì„œì˜ í™•ë¥ ê°’ì„ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê·¼ì‹œì•ˆì ìœ¼ë¡œ í˜„ì¬ timestepì—ì„œ ê°€ì¥ ì¢‹ì•„ë³´ì´ëŠ” ë‹¨ì–´ë¥¼ ì„ íƒí•˜ëŠ” í˜•íƒœë¥¼ `Greedy approach`ë¼ê³  í•œë‹¤.

- Greedy decoding has no way to undo decisions!
	- input : il a mâ€™entartÃ© (he hit me with a pie)
		- he \___
		- he hit \___
		- he hit `a` \___ ~~(whoops, no going back nowâ€¦)~~

ìœ„ ì˜ˆì‹œì—ì„œ `hit` ë‹¤ìŒ `me`ê°€ ë‚˜ì™€ì•¼í•  ì°¨ë¡€ì— `a`ê°€ ë‚˜ì™”ë‹¤ê³  í–ˆì„ ë•Œ, ì´ì–´ ë‚˜ì˜¤ëŠ” ëª¨ë“  ì˜ˆì¸¡ì€ í‹€ì–´ì§€ê²Œ ëœë‹¤. ê·¸ ì´ìœ ëŠ” ë’¤ì—ì„œ ì‚¬ìš©í•´ì•¼í•  ë‹¨ì–´ë¥¼ ì´ë¯¸ ì•ì—ì„œ ìƒì„±í•´ë²„ë ¸ê¸° ë•Œë¬¸ì´ë‹¤. ì´ë¯¸ ë’¤ëŠ¦ê²Œ ê¹¨ë‹¬ì•˜ë‹¤ê³  í•˜ë”ë¼ë„ ì´ë¯¸ ê³ ì •ëœ ì˜ˆì¸¡ê°’ì€ ë°”ê¿€ ìˆ˜ ì—†ë‹¤. 



### Exhaustive search

***Then how can we fix this?***

- Ideally, we want to find a (length $T$) translation $y$ that maximizes. 
	$$
	\begin{align}
	P(y\vert x)
	&=P(y_1\vert x)P(y_2\vert y_1, x)P(y_3\vert y_2, y_1, x)\dots P(y_T\vert y_1, \dots , y_{T-1}, x)\\
	&= \prod_1^TP(y_t\vert y_1, \dots , y_{t-1}, x)
	\end{align}
	$$

- We could try computing **all possible sequences** $y$

	- This means that on each step $t$ of the decoder, we are tracking $V^t$ possible partial translations, where $V$ is the vocabulary size
	- This $O(V^t)$ complexity is far too expensive!

ì¦‰, joint probabilityì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ ê°ê°ì˜ ìµœëŒ€ë¥¼ ê³±í•œ ê°’ì´ ì „ì²´ì˜ ìµœëŒ€ëŠ” ì•„ë‹ ìˆ˜ë„ ìˆë‹¤ëŠ” ê²ƒì´ë©° ì•ì—ì„œ ì•½ê°„ì˜ ì†í•´(ì‘ì€ í™•ë¥ ê°’)ë¥¼ ë³´ë”ë¼ë„ ë’¤ì—ì„œ ë§ŒíšŒí•  ìˆ˜ ìˆëŠ” í™•ë¥ ê°’ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì´ ì•„ì´ë””ì–´ì˜ í•µì‹¬ì´ë‹¤. 

í•˜ì§€ë§Œ, ë§¤ timestepë§ˆë‹¤ ëª¨ë“  ê°€ì§“ìˆ˜ë¥¼ ê³ ë ¤í•œë‹¤ë©´ ë„ˆë¬´ ë§ì€ ê³„ì‚°ëŸ‰ì´ í•„ìš”í•˜ë‹¤. 



### Beam search

ë””ì½”ë”ì˜ ë§¤ íƒ€ì„ë§ˆë‹¤ ì ì ˆí•œ $k$ê°œì˜ ê°€ì§“ìˆ˜ë¥¼ ê³ ë ¤í•˜ê³ , ê·¸ ì¤‘ì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ê²ƒì„ íƒí•˜ëŠ” ë°©ë²•ì´ë‹¤. 

> Core idea: on each time step of the decoder, we keep track of the $k$ most probable partial translations (which we call hypothese)
>
> - $k$ is the beam size (in practice around 5 to 10)
>
> A hypothesis $y_1, \dots, y_t$ has a score of its log probability:
> $$
> score(y_1, \dots, y_t) = logP_{LM}(y_1, \dots, y_t\vert x)=\sum^t_{i=1}logP_{LM}(y_i\vert y_1, \dots, y_{t-1},x)
> $$
> Scores are all negative, and a higher score is better  
> We search for high-scoring hypotheses, tracking the top k ones on each step  

logë¥¼ ì”Œì›€ìœ¼ë¡œì¨ ê³±ì…ˆì„ ë§ì…ˆì—°ì‚°ìœ¼ë¡œ ë°”ê¾¸ê³ , ì¶”ì²™í•˜ê¸° ì‰½ë„ë¡ í•˜ëŠ” ê²ƒì´ ëª©í‘œ

- Beam search is not guaranteed to find a globally optimal solution.
- But it is much more efficient than exhaustive search!

ëª¨ë“  ê²½ìš°ë¥¼ ë‹¤ ë³´ëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, í›„ë³´êµ°ì„ ì¶”ë¦¬ê¸° ì‰½ë‹¤.!!!

**example)**

![beamsearch](../../assets/img/boostcamp/beamsearch.gif){:width="80%"}{:.center}

> ì¶œì²˜ : [https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf](https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf)

ìœ„ì™€ ê°™ì´ k=2ì¼ ë•Œ, ê³„ì†ì ìœ¼ë¡œ í™•ë¥ ì´ ë†’ì€ í›„ë³´êµ°ì„ ì°¾ìœ¼ë©´ì„œ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•œë‹¤. 

### Beam search: Stopping criterion

- Usually we continue beam search until:
  - We reach timestep ğ‘‡ (where ğ‘‡ is some pre-defined cutoff), or 
  - We have at least ğ‘› completed hypotheses (where ğ‘› is the pre-defined cutoff)



### Beam search: Finishing up

- We have our list of completed hypotheses
- How to select the top one with the highest score?
- Each hypothesis $ğ‘¦_1, â€¦ , ğ‘¦_t$ on our list has a score

$$
score(y_1, \dots, y_t) = logP_{LM}(y_1, \dots, y_t\vert x)=\sum^t_{i=1}logP_{LM}(y_i\vert y_1, \dots, y_{t-1},x)
$$

- Problem with this : **longer hypotheses have lower scores**
- Fix : Normalize by length

$$
score(y_1, \dots, y_t) = \frac{1}{t}\sum^t_{i=1}logP_{LM}(y_i\vert y_1, \dots, y_{t-1},x)
$$



## BLEU score

ìì—°ì–´ ìƒì„± ëª¨ë¸ì—ì„œ ìƒì„± ëª¨ë¸ì˜ í’ˆì§ˆ ë° ê²°ê³¼ë¥¼ í‰ê°€í•˜ëŠ” scoring ë°©ì‹ì„ ì•Œì•„ë³´ì



